{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b072e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import lit, desc, col, size, array_contains, isnan, udf, hour, array_min, array_max, countDistinct\n",
    "\n",
    "from pyspark.ml  import Pipeline     \n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need to create a spark container by calling SparkSession. \n",
    "This step is necessary before doing anything\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Example01\").getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Due to parallel execution on all cores on multiple machines, \n",
    "PySpark runs operations faster then pandas. \n",
    "In other words, pandas DataFrames run operations on a single node \n",
    "whereas PySpark runs on multiple machines. \n",
    "\"\"\"\n",
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f37bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use of groupBy function in PySpark\n",
    "#get the sum\n",
    "df.groupBy(\"state\").sum(\"salary\").show(truncate=False)\n",
    "\n",
    "#get the mean\n",
    "df.groupBy(\"state\").mean(\"salary\").show(truncate=False)\n",
    "\n",
    "# check with groupby of departments and the data statistics with bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupBy(\"department\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5784b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupBy(\"department\").min(\"salary\").show()\n",
    "\n",
    "df.groupBy(\"department\").mean( \"salary\").show()\n",
    "df.groupBy(\"department\").mean( \"salary\").show(truncate=False)\n",
    "\n",
    "#replace dept by state and salary by bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#GroupBy on multiple columns\n",
    "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\",\"state\").sum(\"salary\").show()\n",
    "df.groupBy(\"department\",\"state\").mean(\"bonus\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import sum,avg,max\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7403207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce251964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85d423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6fe802",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '../../../Lectures/BMS/DOM305/csv_for_glob/Tab_7.3.1_outlay_expenditure.csv'\n",
    "df1 = spark.read.csv(str(file1),header=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df1.count()\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3932b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show statistic of the data we want\n",
    "df1.describe('Social Services').show()\n",
    "\n",
    "#convert spark dataframe to pandas dataframe\n",
    "dfpandas = df1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb27c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy(\"State/UTs\") \\\n",
    "    .agg(sum(\"Social Services\").alias(\"sum_SS\"), \\\n",
    "         avg(\"Social Services\").alias(\"avg_SS\"), \\\n",
    "         sum(\"Energy\").alias(\"sum_Energy\"), \\\n",
    "         max(\"Energy\").alias(\"max_Energy\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd75d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = '../../../Lectures/BMS/DOM305/csv_for_glob/'\n",
    "df2 = spark.read.csv(str(file2),header=True)\n",
    "df2.show()\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas2 = df2.toPandas()\n",
    "dfpandas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupBy(\"State/UTs\").agg(sum(\"Social Services\").alias(\"sum_SS\"), \\\n",
    "         avg(\"Social Services\").alias(\"avg_SS\"), \\\n",
    "         sum(\"Energy\").alias(\"sum_Energy\"), \\\n",
    "         max(\"Energy\").alias(\"max_Energy\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018fbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
